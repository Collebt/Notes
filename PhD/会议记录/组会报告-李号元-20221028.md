TransGeo 



1p：大家下午好，这次有我来为大家分享关于跨模态图像匹配的工作，这是一篇将transformer用于跨视角图像特征提取，并用于视觉检索定位的工作，被2022cvpr接收。



2p：首先先介绍任务的视觉匹配定位的任务和应用，以及相关工作，再介绍本次分享的文章和相关实验结果。

3p：我们今天将的论文要做的任务是基于图像的视觉地理定位，主要目标就是在地面拍摄一张图片，然后和已有卫星数据库上的图像进行检索和匹配，找到卫星对同一个地点拍摄的图像，以此实现自身的地理坐标的定位。



4p：这个任务主要应用于在gps较弱以及受干扰的情况下，通过视觉的方式进行辅助导航和目标定位。比如在城市中gps受干扰严重，误差会到达50m左右，这是结合视觉定位就能够提升定位的效果。



5p：通常视觉定位的主要流程分为两个阶段，第一阶段首先是将拍摄的视觉图像作为查询图像和已有的数据库的图像进行搜寻，完成图像级的检索，随后将检索到的图像作为同一个地点的大概图像，通过像素级或者语义级的配准，来估计查询图像的几何位置。



6p：整个流程的方法大致分成以下三个部分，第一部分是做图像级检索，主要的工作包括跨视角跨模态的图像级检索，其中较多发布的数据集和工作都集中在街景地面图和卫星光学图像的匹配，第二部分是完成检索后进行几何的位置的估计，包括语义或像素级的配准，2d-3d的匹配和位姿估计等工作。还有一些工作是结合深度学习构建端到端的定位框架，输入查询图像和卫星数据库，直接估计出图像对应的经纬坐标和相机位姿。

这次分享的论文主要是用于地面全景图和卫星光学图像的检索任务。下面介绍全景图和卫星光学图的相关工作。这部分工作重心集中在如何在街景图和卫星图之中提取出共同的特征。



7p：如左边图所示，框架的主要流程就是输入两个不同视角的图像，用网络输出图像的嵌入特征，通过计算特征之间的像素度进行排序，将最高相似度的特征作为检索的结果。CVM-Net比较简单粗暴，提出用孪生网络去提取两个视角上的共同特征，并且使用改进版的triplot loss对输出的特征进行监督。triplet loss 就是减少匹配特征之间的距离，增加不匹配特征之间的距离。后面介绍loss的时候会详细说明。



（VLAD是传统方法，用特征描述的方法构成词库，将不同地方的图像分类到词库的各个编码中）



8p：另一个工作考虑到将地面全景图用gan生成一个对应的空间图，然后再输入网络和卫星图进行匹配检索。这个工作主要的创新点是考虑从街景变换到卫星图。（iccv19）



9p：同一年另一篇工作SAFA则是相反，他的工作就是将卫星图通过极变换生成仿真的地面图片。再进行检索匹配。这一类工作都是变换单方面的视角，通过人为先验拟合出另一个视角的图像风格。同时也会面临一些问题，比如拍摄照片时候的朝向变化的时候，生成图像也会发生对应变换，因此这一类工作通常都需要加入拍摄朝向的先验信息。



10p：同时还有一种工作就是人工对朝向和位置进行编码，让网络从编码中学习到对应的特征信息。同样的这也是需要人为提供先验的位置朝向信息。



11p：以往的工作属于做图像级检索的，但是检索之后还是需要考虑优化检索结果和验证坐标等流程。而检索的结果是离散的，和连续的坐标是有差距的。因此到了21年cvpr，第一篇端到端的gps坐标估计框架提出来了，通过一个mlp去预测偏移量，实现在检索结果中预测坐标的估计。



12p：之前的工作可以看到主要针对两个方面做创新，一点是视图的变换和生成，另一点是对图像做位置编码。因此21年随着transformer的大火，这篇工作l2ltr（layer-to-layer-transformer）就考虑到transformer对位置编码的优势，用transform作为特征的encoder，用于全景图和卫星图的图像特征提取，这是第一篇用transform做这方向的工作。但这一篇工作依旧是用cnn作为backbone输出嵌入的特征。



13p：总结以上的工作，可以看到主要解决的问题其实就是视角的变换和进行位置编码的策略。这时候我们将cnn和transform进行一下对比，可以发现transformer在这个任务上是有天然的优势的。总结一下以往cnn的工作，cnn并没有很好地解决视角变换这个问题，比如由于卷积滑窗是隐式编码了位置信息，在视角不同的情况下，两个视角之间其实是不具有平移不变性的关系的。因此这个限制就需要人们利用先验强行将两个视角变成具有一样的平移不变性关系，那就是先验的变换，包括gan生成卫星图以及用极变换生成街景图，但这时候同时也引入了新的问题。这种变换的人为先验虽然简单，但是过于直接粗暴了，想实现变换还需要加入朝向信息和人为的位置编码。



14p：这时候再来看transform的优势，他可以直接去显式编码位置信息，同时用注意力机制，获得全局像素对特征的关系。 最后一点则是基于这个注意力，可以做到非全局的裁剪，也就是说可以放大网络更关注的地方，对无关紧要的信息可以进行忽略。



15p: 对比了transform的优势之后，作者就想用它在这个任务上解决视角变化和位置编码的问题。1.作者认为街景图和卫星图之间的视角变化不一定就是人为制定的polar 变化或者其他几何变化，而是想网络自己去学习一个合适的变化，这个是通过注意力机制来实现。 2. 作者同样也想借助transform对位置编码的优势，想让网络自己学习一个位置编码而不是人为固定朝向。3. 作者认为有些区域是视角特有的，比如说卫星图的屋顶和街景图的天空，这些在另一个视角中是无法观测到的，作者希望设计一种裁剪方法去减少他们的影响。



16p：基于上面三个目的，作者提出了一个完全基于transformer的跨视角图像检索框架。 如图所示，主要框架分成两个分支：对地面全景图提取特征的分支和对卫星图提取特征的分支。里面的网络架构是经典的transformer encoder结构。里面包括多层注意力和mlp，最终输出图像对应的embedding feature。



17p：首先介绍用于街景图处理的分支，1. 首先对图像进行裁剪，得到n个pxp像素的patches，然后每一个patch进过一层线性变化得到每个patch的token，2. 同时为了增强图片的分类效果，还加入了BETR（nlp）的class token，加强不同场景之间特征的区分性。 3. 此外每一个token会加入对应的位置编码 position embedding， 这个位置编码是learnable的，后面会有实验对比可学习的位置编码和用sin变换的位置编码之间的性能区别。 4. 最后经过transformer encoder 和mlp得到的一个 高维的embedding feature。



18p：另一个分支是对卫星图进行特征提取，大致流程和街景图一样，但这里增加了一个阶段，用于将图像放大和聚焦。作者受到人类看地图习惯的启发，人们看地图会通过放大和聚焦来获得更准确的位置，比如建筑的屋顶是没有必要的，可以将其去除，同时还能放大图像，将重要的像素放大，而重要性则来自transform最后一层的注意力权重上。因此第一阶段首先对卫星图进行初步的特征提取，然后在encoder最后一层获得这个图像的注意力权重，根据注意力权重去裁剪和放大图像。将处理后的图像再一次输入网络，提取得到一个更加精准的特征。



19p：这个attend and zoom in的机制过程如图： 最左边是encoder输出的每一个token的权重，将它还原成图像的原始patch位置后，可以看到一些部分权重较大的patch，这些部分就保留下来，并设置一个beta的比例保留高权重的patch。而权重较低的patch就被裁剪掉，不参与以后的计算。同时还按照gammma的比例放大图像，可以得到更清晰的分辨率，提高特征的区域分辨率。



其中红色的box代表的是class token，他不参与裁剪。黑色的box代表去除的patch，不参与后续的运算，这样可以减少运算成本。



20p：最后一共有三个图像encode成特征，这是参照cvm-net的soft-marin triplet loss。拉近同一个目标特征之间的距离，圆圈代表街景的特征，三角代表卫星图的特征。同样颜色代表了对同一个目标的图像，绿色就是拉近两者的距离，红色要拉开负样本之间的距离。一共可以有nx（n-1）对loss，同时因为卫星图有两个分支，因此loss数量乘2.



21p：可以看到整个框架相对于以往的cnn，其实使用transformer代替了人为先验的视角变换，并且加入了位置编码和裁剪放大模块增强性能。



22p：接下来是实验部分，首先介绍两个数据集，cvusa和vigor，cvusa主要收集suburban的图像，vigor收集美国四个城市的卫星图和街景。



23p：实验的指标有以下三个：最主要就是topk的召回精度指标，代表了真实匹配的图像在检索排序预测前k的准确率。

另外vigor由于有4个城市，因此可以分成same-area和cross-area两个部分，（介绍）。另一个是位置度量指标，由于vigor提供了街景的gps坐标，因此可以比较检索出的图像和真实值在坐标距离上的差距（将检索到的卫星图坐标作为预测街景图的坐标）代表预测的坐标误差在多少米以内的准确率。



24p：对比以往的算法，效果都比较好。t标志的代表用极变换的方法

25p：在viogr上比较meter-level的准确率，vigor在更精确距离上表现更好，因为有offset模块的加成，因此对比了没有offset模块的viogr。



26p：下面是消融实验，首先验证加极变换的效果，极变换可以有效增加cnn的方法的性能，但是对transform的增加不多。说明transform的位置编码有了一部分这个功能。另一个是位置编码用可学习的编码方式性能更好。



27p：对比cropandzoom in的效果，crop之后性能稍微下降，但是影响不大。gamma提升分辨率后让patch的数量一样时，性能得到提升。



28P：说明

















