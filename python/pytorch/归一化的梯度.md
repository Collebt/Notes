# max-min  normalize backward in pytorch



在数据处理中，有种不太常用的归一化操作：max-min 归一化，将指定集合的元素归一化到[0,1]之间
$$
h = \frac{x - x_{min}}{x_{max} - x_{min}}
$$
在代码中的操作为：

```python
A -= A.min(1, keepdim=True)[0]
A /= A.max(1, keepdim=True)[0]
```



在梯度反传的计算是：

- 在pytorch的autograd中，max，min的反传只关注对应的元素，其他元素的梯度为0，但是仍然会加入计算图中（grad=0），而直接用python的max，是不产生梯度的（grad=None）



在考虑整个集合的梯度时，应该分成三类：最大值的元素，最小值的元素，其他元素。

#### 最大化：

$$
\f{\partial h }{\partial x}
$$

